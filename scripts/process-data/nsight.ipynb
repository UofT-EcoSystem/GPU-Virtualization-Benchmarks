{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import argparse\n",
    "import cxxfilt\n",
    "import re\n",
    "import difflib\n",
    "\n",
    "from primitives.draw_bar import *\n",
    "\n",
    "\n",
    "def parse_csv(filename, t):\n",
    "    # find out number of lines to skip in file\n",
    "    n_skip = 0\n",
    "    output_exists = False\n",
    "\n",
    "    with open(filename) as search:\n",
    "        for num, line in enumerate(search, 1):\n",
    "            # Sketchy, might break\n",
    "            if '\\\"Metric Value\\\"' in line or '\\\"Duration\\\"' in line:\n",
    "                n_skip = num - 1\n",
    "                output_exists = True\n",
    "                \n",
    "    if not output_exists:\n",
    "        print(filename, \"went wrong!\")\n",
    "        return False, None\n",
    "\n",
    "    print(\"Parsing\", filename)\n",
    "\n",
    "    print(n_skip)\n",
    "    # read data using pandas\n",
    "    if t == 'time':\n",
    "        skip_call = lambda x: x in range(n_skip) or x == (n_skip + 1)\n",
    "    else:\n",
    "        skip_call = lambda x: x in range(n_skip) \n",
    "\n",
    "    data = pd.read_csv(filename, delimiter=',', skiprows=skip_call, thousands=',')\n",
    "\n",
    "    # return a map of metric type to pandas dataframe\n",
    "    return True, data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing /home/serinatan/project/GPU-Virtualization-Benchmarks/benchmarks/rodinia/scripts/results-all/time/leukocyte.txt\n",
      "3\n",
      "Parsing /home/serinatan/project/GPU-Virtualization-Benchmarks/benchmarks/rodinia/scripts/results-all/inst/leukocyte.txt\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "success, df_time = parse_csv('/home/serinatan/project/GPU-Virtualization-Benchmarks/benchmarks/rodinia/scripts/results-all/time/leukocyte.txt', 'time')\n",
    "success, df = parse_csv('/home/serinatan/project/GPU-Virtualization-Benchmarks/benchmarks/rodinia/scripts/results-all/inst/leukocyte.txt', 'inst')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_kernel(kernels):\n",
    "    # list out the kernel names\n",
    "    plt.subplots_adjust(bottom=0.62)\n",
    "    for i in np.arange(kernels.shape[0]):\n",
    "        dm = cxxfilt.demangle(kernels[i])\n",
    "        name = dm\n",
    "        name = re.sub(\">\\((.+)\\)$\", '>', dm, 1)\n",
    "        #name = re.sub(\"^([a-z]+)\\s\", '', name, 1)\n",
    "        plt.text(0.1, 0.6-i/kernels.shape[0]*0.4, '{}: {}'.format(i, name), transform=plt.gcf().transFigure)\n",
    "\n",
    "def time(time_df):\n",
    "    # get the total runtime\n",
    "    total = time_df.iloc[-1]['Start'] + time_df.iloc[-1]['Duration'] - time_df.iloc[0]['Start']\n",
    "\n",
    "    # drop rows with NaN (cudamemset has NaN grid size)\n",
    "    time_df = time_df.dropna(subset=['Grid X'])\n",
    "    print('num invoks:{}'.format(time_df.shape))\n",
    "\n",
    "    # group duration of the kernels\n",
    "    time_df = time_df[['Name', 'Duration']]\n",
    "    result = time_df.groupby(['Name'], as_index=False).sum()\n",
    "    result['Duration'] = result['Duration'] / total * 100\n",
    "    result.rename(columns={'Name': 'Kernel Name', 'Duration':'Importance'}, inplace=True)\n",
    "    print('kernels in time df: {}'.format(result['Kernel Name'].unique().shape))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def join(df_time, df):\n",
    "    # kernel names produced by nvprof and nsight are slightly different \n",
    "    # need to manually equate them\n",
    "    col_import = []\n",
    "    matches = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # append importance to list\n",
    "        matched = df_time[df_time['Kernel Name'].str.contains(row['Kernel Name'])]\n",
    "        if matched.size == 0:\n",
    "            print('No matching kernel in time df!')\n",
    "            print(row['Kernel Name'])\n",
    "            #exit(1)\n",
    "            col_import.append(0)\n",
    "        else:\n",
    "            if matched.shape[0] != 1:\n",
    "                print('{} has {} matches'.format(row['Kernel Name'], matched.shape[0]))\n",
    "            matches.append(matched)\n",
    "            col_import.append(matched['Importance'].sum())\n",
    "\n",
    "    result = df\n",
    "    result['Importance'] = col_import\n",
    "    result = result.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # debug: find out which ones are leftover from time\n",
    "    match_time = pd.concat(matches)\n",
    "    diff = pd.concat([match_time, df_time]).drop_duplicates(keep=False)\n",
    "    print(diff['Kernel Name'])\n",
    "\n",
    "    # xticks with importance\n",
    "    xticks = ['{}\\n{:.1f}%'.format(i, result.iloc[i]['Importance']) for i in range(result.shape[0])]\n",
    "\n",
    "    # drop rows with importance below 1%\n",
    "    neglible = result[ result['Importance'] < 1 ].index\n",
    "    result.drop(neglible, inplace=True)\n",
    "\n",
    "    # drop the importance column\n",
    "    result = result.drop(columns='Importance')\n",
    "\n",
    "    return result, xticks\n",
    "\n",
    "def instmix(df_time, df, bench_name):\n",
    "    #bench_name = args.name\n",
    "    #outdir = args.outdir if args.outdir else args.indir\n",
    "\n",
    "    # divide by 1 million\n",
    "    df['Metric Value'] /= 1000000\n",
    "\n",
    "    # process each benchmark individually\n",
    "    k_metrics = df.groupby(['Kernel Name','Metric Name'], as_index=False)[['Metric Value']].sum()\n",
    "    #k_metrics = k_metrics.reset_index(level=['Kernel Name','Metric Name'])\n",
    "\n",
    "    df_map = {}\n",
    "    df_map['Kernel Name'] = k_metrics['Kernel Name'].unique()\n",
    "    print('kernels in inst:{} '.format(df_map['Kernel Name'].size))\n",
    "\n",
    "    metrics = ['FP16', 'FMA', 'FP64', 'INT', \n",
    "            'SPU', 'Tensor', 'Load/Store', 'EXEC' ]\n",
    "\n",
    "    # I really hope no one sees this: re-organize the table\n",
    "    for metric in metrics:\n",
    "       metric_col = k_metrics.loc[k_metrics['Metric Name'] == metric]['Metric Value'].array\n",
    "       df_map[metric] = metric_col\n",
    "\n",
    "    trans_df = pd.DataFrame(df_map)\n",
    "\n",
    "    # inner join the two tables: importance and metric values\n",
    "    # also sort the table rows by descending importance\n",
    "    df_join, xticks = join(df_time, trans_df)\n",
    "    \n",
    "        \n",
    "    df_join = df_join.sum(axis=0)\n",
    "    df_join['Kernel Name'] = bench_name\n",
    "\n",
    "    # normalize each benchmark mixture and scale it by importance\n",
    "    df_join[metrics] = df_join[metrics].div(df_join['EXEC'], axis=0)\n",
    "    #.multiply(df_join['Importance'], axis=0)\n",
    "                                       \n",
    "    metrics.remove('EXEC')\n",
    "    print(df_join[metrics])\n",
    "    df_join['Other'] = df_join['EXEC'] - (df_join[metrics].sum())\n",
    "\n",
    "    cols = ['Kernel Name', 'FP16', 'FMA', 'FP64', 'INT', \n",
    "            'SPU', 'Tensor', 'Load/Store', 'Other' ]\n",
    "    df_join = df_join[cols]\n",
    "\n",
    "    results = df_join.values\n",
    "\n",
    "    # set graph size\n",
    "    #plt.rcParams[\"figure.figsize\"] = (30, 40)\n",
    "\n",
    "    # print the list of kernels\n",
    "    #print_kernel(results[:, 0])\n",
    "\n",
    "    #legends = ['FP16', 'FMA', 'FP64', 'ALU', 'SPU', 'Tensor', 'Ld/St', 'Other']\n",
    "    #draw_stack(results, legends, bench_name.capitalize(), 'Kernel ID', \n",
    "            #'Normalized Instruction Count Scaled by Runtime Weight',\n",
    "            #xticks=xticks,\n",
    "            #outfile=os.path.join(outdir, 'inst.pdf'))\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num invoks:(7, 20)\n",
      "kernels in time df: (3,)\n",
      "kernels in inst:3 \n",
      "Series([], Name: Kernel Name, dtype: object)\n",
      "FP16                  0\n",
      "FMA            0.467613\n",
      "FP64          0.0657904\n",
      "INT            0.221578\n",
      "SPU           0.0381778\n",
      "Tensor                0\n",
      "Load/Store    0.0309369\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['leu', 0.0, 0.4676126648215417, 0.06579036050282383,\n",
       "       0.2215778803084024, 0.03817776500970491, 0.0, 0.030936920277429173,\n",
       "       0.17590440908009786], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = time(df_time)\n",
    "\n",
    "instmix(dft, df, 'leu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp(args, df_time, df):\n",
    "    bench_name = args.name\n",
    "    outdir = args.outdir if args.outdir else args.indir\n",
    "\n",
    "    # process each benchmark individually\n",
    "    k_metrics = df.groupby(['Kernel Name','Metric Name'], as_index=False)[['Metric Value']].mean()\n",
    "    #k_metrics = k_metrics.reset_index(level=['Kernel Name','Metric Name'])\n",
    "\n",
    "    df_map = {}\n",
    "    df_map['Kernel Name'] = k_metrics['Kernel Name'].unique()\n",
    "\n",
    "    metrics = ['FP16', 'FMA', 'FP64', 'ALU', 'SPU', 'Tensor', 'Load/Store']\n",
    "\n",
    "    # I really hope no one sees this: re-organize the table\n",
    "    for metric in metrics:\n",
    "        metric_col = k_metrics.loc[k_metrics['Metric Name'] == metric]['Metric Value'].array\n",
    "        df_map[metric] = metric_col\n",
    "\n",
    "    trans_df = pd.DataFrame(df_map)\n",
    "    cols = ['Kernel Name'] + metrics\n",
    "    trans_df = trans_df[cols]\n",
    "\n",
    "    df_join, xticks = join(df_time, trans_df)\n",
    "\n",
    "    #df_join = df_join[cols]\n",
    "\n",
    "    results = df_join.values\n",
    "\n",
    "    # set graph size\n",
    "    plt.rcParams[\"figure.figsize\"] = (30, 20)\n",
    "\n",
    "    legends = ['FP16', 'FMA', 'FP64', 'ALU', 'SPU', 'Tensor', 'Ld/St']\n",
    "    draw_bar(results, legends, bench_name, 'Kernel ID', \n",
    "            'Utilization during Active Cycles (%)',\n",
    "            xticks=xticks,\n",
    "            outfile=os.path.join(outdir, 'comp.pdf'))\n",
    "\n",
    "def mem(args, df_time, df):\n",
    "    bench_name = args.name\n",
    "    outdir = args.outdir if args.outdir else args.indir\n",
    "\n",
    "    # process each benchmark individually\n",
    "    k_metrics = df.groupby(['Kernel Name','Metric Name'], as_index=False)[['Metric Value']].mean()\n",
    "    #k_metrics = k_metrics.reset_index(level=['Kernel Name','Metric Name'])\n",
    "\n",
    "    df_map = {}\n",
    "    df_map['Kernel Name'] = k_metrics['Kernel Name'].unique()\n",
    "\n",
    "    metrics = ['DRAM_UTIL_PCT', 'REGISTER_USAGE', 'DYNAMIC_SHARED_USAGE', \n",
    "            'STATIC_SHARED_USAGE', 'L2 Hit Rate', 'OCCUPANCY', \n",
    "            'L1 Hit Rate', 'Block Size', 'Shm Config Size']\n",
    "\n",
    "    # I really hope no one sees this: re-organize the table\n",
    "    for metric in metrics:\n",
    "        metric_col = k_metrics.loc[k_metrics['Metric Name'] == metric]['Metric Value'].array\n",
    "        df_map[metric] = metric_col\n",
    "\n",
    "    trans_df = pd.DataFrame(df_map)\n",
    "    cols = ['Kernel Name'] + metrics\n",
    "    trans_df = trans_df[cols]\n",
    "\n",
    "    # adjust memory usage according to total available amount\n",
    "    trans_df['REGISTER_USAGE'] = trans_df['REGISTER_USAGE'] * 1024 * trans_df['OCCUPANCY'] / (64 * 1024)\n",
    "    blocks_per_sm = trans_df['OCCUPANCY'] * 32 * 32 / trans_df['Block Size']\n",
    "    trans_df['SHARED_USAGE'] = (trans_df['DYNAMIC_SHARED_USAGE'] \n",
    "                               + trans_df['STATIC_SHARED_USAGE'] ) * (blocks_per_sm) / trans_df['Shm Config Size']\n",
    "\n",
    "    #'L2 Hit Rate' is bogus, bug in nsight\n",
    "    cols = ['DRAM_UTIL_PCT', 'REGISTER_USAGE', 'SHARED_USAGE', \n",
    "             'OCCUPANCY', 'L1 Hit Rate']\n",
    "    trans_df = trans_df[['Kernel Name'] + cols]\n",
    "\n",
    "    df_join, xticks = join(df_time, trans_df)\n",
    "    #df_join = df_join.drop(columns='Kernel Name')\n",
    "\n",
    "    results = df_join.values\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (35, 20)\n",
    "\n",
    "    draw_bar(results, cols, bench_name, 'Kernel ID', \n",
    "            'Memory Utilization during Active Cycles (%)',\n",
    "            xticks=xticks,\n",
    "            outfile=os.path.join(outdir, 'mem.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['two', 'three', 'one']\n",
    "a.insert(0, a.pop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', 'two', 'three']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
